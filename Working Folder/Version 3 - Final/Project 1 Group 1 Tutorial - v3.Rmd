---
title: "Group 1 Project 1 Tutorial"
authors: "Jayden Benzan, Rebecca Beneroff, Kayla Lichtner"
date: "2024-09-29"
output: github_document
---
Tutorial: An Introduction to Data Preparation, Exploration, and Analysis

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Tutorial introduction 

Do you have a dataset but you're not sure where to start with analysis? This tutorial is for you! Utilizing an Iris data set, you will start by learning how to import, classify, and organize your data to prepare it for use. You will then perform some base level data exploration by looking at your variable's distributions assessing for normalcy, and transforming as necessary. Finally, you will develop and explore a question about the relationships between the variables presented in the data. Using this question, you will learn some of the different statistical tests that can be run, why these tests are run, and data visualization. 

## Tutorial objectives

There are three objectives for this tutorial:

1. Understand how to classify data (understanding different variable types and its effect on how to use them in analysis).
2. Review loading data, organizing data, and preparing data for statistical analysis.
3. Explain the conditions required for using linear modeling, t.tests, or ANOVA tests to analyze data. 

## Data introduction 

The data set used in this tutorial contains data for members of three different species of Iris. This is an expanded version of a very popular introductory dataset. It features morphometric data highlighting the petals and sepals of *Iris* flowers. Petals being the colored modified leaves that surround the reproductive parts of the flower and sepals being the typically green leaf like structures that protect the flower bud and provide support for the petals when the flower is in bloom. The data set also records the elevation and soil type where the flowers are found. There are 1200 observations in the data set with 400 observations per species. 


# Getting Set up

## Loading the necessary packages 

```{r}
if (!require("UsingR")) install.packages("UsingR"); 
  library(UsingR) # For the simple.eda function
if (!require("GGally")) install.packages("GGally");
if (!require("cowplot")) install.packages("cowplot"); 
  library(cowplot) # For graphing aesthetics
if (!require("conflicted")) install.packages("conflicted"); 
  library (conflicted) # For conflicts
if (!require("readxl")) install.packages("readxl"); 
  library(readxl) #To read in data
if (!require("gplots")) install.packages("gplots"); 
  library(gplots) #For graphs/plots
if (!require("tidyverse")) install.packages("tidyverse"); 
  library(tidyverse) # Always download this!
conflict_prefer_all("dplyr", quiet = TRUE)
```

## Importing the dataset

Go to our Git repository called Project_1_Group_1_2024, and download the "iris.extended.csv" data file from the "Dataset options" folder. We are going to make a final folder with the finished tutorial and the data csv file, but for now, please do this to import the data.

```{r}
iris.raw.data <- read.csv("iris_extended.csv")
```

Alternatively, you can use this script to download the data set from online:

```{r}
library (readr)
iris.raw.data <- read.csv(url("https://raw.githubusercontent.com/Bucknell-Biol364/Project_1_Group_1_2024/refs/heads/main/Final%20Folder/iris_extended.csv"))
```

*Citation for the dataset: https://www.kaggle.com/datasets/samybaladram/iris-dataset-extended?select=iris_extended.csv*


After loading in a dataset, you always want to make sure that everything loaded in properly. To do this, check the top and bottom of your data, and the number of observations you have imported. The data on GitHub reports 1200 lines of data, in our environment the data should read 1200 observations as there is one line for column headings.

```{r}
head(iris.raw.data)
tail(iris.raw.data)
nrow(iris.raw.data)
```

This shows us that there are 1200 rows of data which aligns with the expectation of 1200 lines of data according to Github.

It is important to understand the structure of the raw data as it will impact how you prepare for analysis. To do so:

```{r}
glimpse(iris.raw.data)
```

# Looking into different variables 

If you are ever looking at a data set and you find it hard to understand what type of variables you are looking at here is a few simple ways to get a bettering understanding. In th code chuck above there is a code used called "glimpse". There are typically two variables that we look at when conducting data exploration which are response and explanatory.

**Explanatory** - Also known as an independent variable or predictor, this variable is used to explain or predict the outcome variable. It's the expected cause of the response variable.

* Ex: Elevation, 

**Response** - Also known as a dependent variable, this variable is the outcome variable that's explained or predicted by the explanatory variable. It's the expected effect of the explanatory variable.

* Ex: 
sepal_length, sepal_width, petal_length, petal_width, sepal_area, , sepal_aspect_ratio, petal_aspect_ratio, sepal_to_petal_length_ratio, sepal_to_petal_length_diff, sepal_to_petal_width_diff, petal_curvature_mm, petal_texture_trichomes_per_mm2, leaf_area_cm2, sepal_area_sprt, petal_area_sprt, area_ratios

**Factoring:** - the process of converting a variable into a factor, which is a data type used to represent categorical data. Factors store both the unique categories (called levels) and the values that belong to those categories. Factoring is commonly used when you want to classify data into distinct groups, such as types, species, or categories, for statistical modeling or analysis.

```{r}
iris.raw.data <- iris.raw.data |> 
  mutate(soil_type = as.factor(soil_type),
         species = as.factor(species))

summary(iris.raw.data$soil_type)
summary(iris.raw.data$species)
```

* Ex: After running this code, both the soil_type and species columns will be stored as factors in the iris.raw.data dataset, making it easier to perform tasks like grouping, summarization, or statistical modeling that depend on categorical variables.


* Where would these go?: 
sepal_to_petal_width_ratio, Soil_type, petal_area, Species


Once you are able to read the data into R your next step would be to use the following code:
`data(iris.raw.data)` .


# Exploratory Data Analysis

It is important to run an initial exploratory data analysis to understand the distributions of your dataset as the distribution of data directly influences the type of statistical analyses to perform. 

## Check for NAs in the data 

We should start be checking for for NAs in the dataset. NAs within the set mean that there are missing data values. Missing data values can make the row containing the NA less useful to us as the missing information could skew our tests. It is important to consider how dropping NAs might affect your analysis, for instance is the NA in a section of the data that you are not looking to analyze? Consider the possible reasons you might want or not want to drop NAs before doing so. 
In this dataset there are no NAs. If there were though and we did want to remove them, we could use the function `drop_na()` to omit those rows.

## Checking Distributions

To explore the distributions of some of the variables we are interested in we can use the simple.eda() function to look at the distributions of the numerical data.

```{r}
simple.eda(iris.raw.data$sepal_area)
simple.eda(iris.raw.data$sepal_length)
simple.eda(iris.raw.data$sepal_width)
simple.eda(iris.raw.data$petal_width)
simple.eda(iris.raw.data$petal_length)
simple.eda(iris.raw.data$petal_area)
```

The results of this preliminary analysis shows that sepal area and sepal width are approximately normally distributed as their histograms follow a bell shaped curve. The other variables sepal length, petal width, petal length, and petal area are not normally distributed as their histograms are skewed and their boxplots are skewed. Also, their data does not align well with the theoretical line in the Q-Q plot. 

To confirm the distributions of the dataset, we are going to use a Shapiro test for normality. To determine normality, if the p-value produced by a Shapiro test is greater than the confidence level alpha = 0.05, the distribution is normal. If the p-value produced by the Shapiro test is less than the confidence level alpha = 0.05, the data is not normally distributed. 

```{r}
shapiro.test(iris.raw.data$sepal_area)
shapiro.test(iris.raw.data$sepal_width)
shapiro.test(iris.raw.data$sepal_length)
shapiro.test(iris.raw.data$petal_length)
shapiro.test(iris.raw.data$petal_area)
shapiro.test(iris.raw.data$petal_width)
```

The results of the Shapiro test show that all six variables do not follow a normal distribution as the p-values produced by the Shapiro test are lower than the confidence level alpha = 0.05. Therefore, to prepare the data for statistical analysis, we will log transform the data to fit the data into a normal distribution.

## Log Transform the Data

```{r}
iris.raw.data$log10sepal_length <- log10(iris.raw.data$sepal_length)
simple.eda(iris.raw.data$log10sepal_length)

iris.raw.data$log10sepal_width <- log10(iris.raw.data$sepal_width)
simple.eda(iris.raw.data$log10sepal_width)

iris.raw.data$log10petal_length <- log10(iris.raw.data$petal_length)
simple.eda(iris.raw.data$log10petal_length)

iris.raw.data$log10petal_width <- log10(iris.raw.data$petal_width)
simple.eda(iris.raw.data$log10petal_width)
```

After log transforming the sepal length data, we find that it now follows a normal distribution. It is important for a dataset to be normally distributed for parametric tests as parametric tests use means to calculate statistical differences. Therefore, means need to be appropriately representative for a given data set, provided by a normal distribution.For instance a mean from a a given population that is skewed is not accurately representative of the data. 

After log transforming the petal length data, we found that it did not convert into a normal distribution. What aspect of the data do think is driving this? After reflection, please see below for an explanation!

## Filter the data by species:

```{r}
setosa_data <- iris.raw.data |> 
  filter(species=="setosa")

versicolor_data <- iris.raw.data |>
  filter(species=="versicolor")

virginica_data <- iris.raw.data |>
  filter(species=="virginica")
```

Now, look at petal length for all three iris species separately:

```{r}
simple.eda(setosa_data$log10petal_length)
simple.eda(versicolor_data$log10petal_length)
simple.eda(virginica_data$log10petal_length)
```
By separating the data by species, we find that the log transformation of petal length is now normally distributed. 


# Running Data Analysis 

## Form a Question:

At this point in your analysis you will be investigating the interactions between variables. You should start by formulating a question you might have about the dataset - For instance, you could choose to focus on one of the 3 species featured in the dataset. Noticing that *Iris setosa* inhabits three different soil types - loamy, sand, and clay - you might want to investigate if there is any relationship between soil type and petal length in *I. setosa*. You could then look for relationships between soil and petal width as well as soil and petal total area. Maybe you want to extend that to elevation too. Once you have a question in mind you can decide what tests might best serve your investigation best. 

* Look at your data again and formulate a question or hypothesis you want to test. This is a large set so I'm going to use a pairplot to see if there are any relationships that catch my interest:

```{r}
library("GGally")

ggpairs(iris.raw.data, # calls ggpairs and tells it to use our iris dataset
  columns = 4:7, # Tells R that we are only looking to visualize colums 4-7 
  aes(color = species, alpha = 0.5), # Tells R that we want to color the graph by the species type so we can visualize the species differences
  upper = list(continuous = "points", position = "jitter")) + # This adds the scatter plots 
  theme_cowplot() #This applies the cowplot theme which makes the plot look a little neater by doing things such as removing the usual grey grid background.
```

The above code first loads GGally which is an eXtension of ggplot2, a preferred package for creating graphs. The ggpairs plot provides as a scatterplot matric that gives us a great initial summary visualization. We're missing some of our variables such as elevation and soil type for the sake of visual clarity but this allows us a quick glance into some of the relationships that might exist within the data set. 


## The T.Test

T.Tests are statistical tests that are used to compare the means between two groups of data. This is useful in  hypothesis testing because we can use a t.test to determine whether or not a factor or treatment actually has a significant effect on a population or group. It can also tell us whether or not two groups are different from one another. This test makes the following assumptions:

1. The data is continuous
2. The data set has been obtained by random samples from a population
3. The variability of the data in each group is similar
4. The distribution is approximately normal

To perform the t.test, you will first form a null hypothesis. The null hypothesis should state that what you are investigating has no significant effect on the population you are investigating. 

In the case of the Iris petals, we will say that Soil type X does not have any affect on the length of *I. setosa* petals. 

We have already filtered our data by species so we can grab our *I. setosa* data. 

Now for the sake of demonstration since we need 2 variables, lets say we're only interested in comparing loamy and sandy soils. We're conducting an experiment in house and we can't replicate clay, so we are only interested in loam and sand. We will then filter out clay from the data set. The | operator acts as "or" which means it will pull out loamy or sandy rows but not clay.

```{r}
setosa_soil_data <- setosa_data |> 
  filter(soil_type=="loamy" | soil_type=="sandy")
summary(setosa_soil_data$soil_type)
```

**Your Turn:**

* Perform any filtering or arranging you might need to do here:

```{r}

```


* State your null hypothesis:

We will now perform the T Test:

```{r}
t.test(log10petal_length ~ soil_type, data = setosa_soil_data)
```

The example t.test returned a p-value of 0.2326. The p-value tells us the likelihood that you might observe the modeled outcome in your data set given that the null hypothesis is true. We will reject the null hypothesis if p < 0.05. Given our test returned a p-value of 0.7936, we can not confidently say amongst clay or sandy soil that soil type affects the length of petals in *I. setosa* so we fail to reject the null hypothesis.

**Your Turn:**

You will now run the t.test. Use the above sepal/soil test code as a guide to test your hypothesis.

```{r}

```


* Interpret your t.test below:



## Analysis of Variance (ANOVA) Test

Anova tests follow the same principle as t-tests but they are used when you have more than 2 groups. It can also be used to compare nested models.

So, returning to our soil and petals test we can add clay back into the mix! We will therefore use our filtered setosa data set rather than the soil filtered set.

Our null hypothesis is that there is no difference in the means of the different layers (petals at each soil type).

```{r}

SepalSoilaov <- aov(setosa_data$log10petal_length~setosa_data$soil_type)
summary(SepalSoilaov)

```
Our p-value is 0.283 and so we again fail to reject the null hypothesis, it does not seem like soil type has a significant effect on petal length.

**Your Turn:**

Perform an Anova test below and interpret your results:

```{r}

```


## Linear Modeling

Linear modeling looks for direct correlations between variables. 
It is a parametric test and assumes: 

1. The relationship between x and the mean of y is linear
2. That the observed data points are independent of each other
3. That the data is normally distributed
4. Homoscedasticity - That the spread of data variance is approximately the same

Lets investigate a new question, since there was no significance in petal/soil data, lets investigate the relationship between petal length and width since it seemed like there might be a linear correlation between the two in the pairplot. We can investigate this with a linear model.

```{r}
lmSentosaPetalLW <- lm(log10petal_length ~ log10petal_width, data = setosa_data)

summary(lmSentosaPetalLW)
```
We have returned a p-value of 0.445 so it does not seem like there is a lot of correlation within *I. sentosa*. Our pair plot did include all of the species, so what does our model look like if we broaden it a little and look at all three of the Iris species?

```{r}
lmIrisPetalLW <- lm(log10petal_length ~ log10petal_width, data = iris.raw.data)

summary(lmIrisPetalLW)
```
This returns a p-value of 2e^-16^ which is below p=0.05 so we can reject the null hypothesis and say that within these three Iris species, there does appear to be correlation. 

**Your Turn:**

Determine if your data fits the requirements for linear modeling and perform your own tests below:

```{r}

```


## Visualization

Now what does this look like? Visualizations are an important part of understanding our data. Visualizations allow us to see a better picture of the story the data tells. It allows us to find identify and recognize trends, outliers, and patterns within the dataset. It is also an important tool for presenting your data to other people. This tutorial will give a brief overview of data visualization. We are going to use the ggplot package, a package dedicated exclusively to data visualization so you can quickly make visually appealing and effective graphs. 

Lets start by making a quick graph to take a look at the relationship between our log transformed petal length and petal width of *I. setosa*.

```{r}
ggplot(setosa_data) + # This calls ggplot and tells it we're using the setosa dataset.
  aes(x = log10petal_length, y=log10petal_width) + # This sets our x and y axis with the variables we are looking to explore
  geom_point(position = "jitter",size=1) + # This sets up our scatter plot and adds jitter which, basically, spreads our points out and makes them easier to see
  theme_cowplot()+ # Good ol' cowplot from before
   geom_smooth(method='lm',se=FALSE) # This adds our trend line

```

This graph confirms what our p-values told us before, there does not appear to be any linear relationship between petal length and width in *I. setosa*.


But we did see a significant p-value when we looked at petal length and width in the whole *Iris* set so lets take a look at that relationship.  

```{r}
ggplot(iris.raw.data) + # Same as before but this time we're calling the entire iris dataset
       aes(x=log10petal_length, y=log10petal_width, color = species) +  # Same as before but now like we did with GGally, we're telling it we want to color the points by the Iris species
  geom_point(position = "jitter",size=1) + # Same as the last graph
  geom_smooth(method = lm, color="black") + # This is the same but this time we are telling it we want to make the trendline black
  theme_cowplot() # And once more, good ol' cowplot

```

Similar to what we saw in the GGally plot, we can see that, as the p-value indicated, there does seem to be a linear relationship between petal width and length between these species. If we were to analyze this further, we could try to see if we could find something that might explain the difference such as looking at the soil and elevation data. There are also many different graphs to make and ways to make them look better such as changing the title and axis names, orienting your axis value positions, and changing the colors of the data. For now, it's...


...**Your Turn!:** 

* What does your data look like? Make your own graph below

```{r}

```


# Summary

### Purpose of each type of test:

**One-way anova** - One-way ANOVA is used to determine if there are statistically significant differences between the means of three or more independent groups. It helps you understand if at least one group differs from the others, but it doesn’t tell you which specific groups differ.

* Ex: You want to compare the mean sepal_length between different Species to check if any species has significantly different sepal lengths.

*Note that If the ANOVA result is significant, post-hoc tests (e.g., Tukey's HSD) can determine which groups are different from each other.*

**t-test** - The t-test is used to compare the means of two groups. There are two common types:

* **Independent t-test** - Compares means between two independent groups (e.g., sepal_length for "setosa" vs. "versicolor").

* **Paired t-test** - Compares means of the same group at different times or conditions.
You have two independent groups (e.g., comparing two species) and a continuous variable (e.g., sepal_length).
  * Ex: Comparing the mean sepal_length between two species.

**simple linear regression** - Simple linear regression models the relationship between two continuous variables. It is used to predict the value of a response variable (dependent) based on the value of an explanatory variable (independent).

You want to predict one continuous variable (e.g., sepal_length) from another continuous variable (e.g., petal_length).

* Ex: You want to predict sepal_length from petal_length.

*Note:*
This will provide an equation for the linear relationship between the two variables, which you can use for prediction. The summary() function will give you important statistics like the slope, intercept, and R-squared value (which shows how much of the variation in the response variable is explained by the explanatory variable).

**simple eda** - Exploratory Data Analysis (EDA) is the process of analyzing data sets to summarize their main characteristics using visualizations and statistical methods. The goal is to better understand the structure of the data, detect patterns, spot anomalies, and test initial hypotheses.

## When to Use: 

At the beginning of a data analysis project to explore your dataset and get a sense of its structure.

* Ex: Summary statistics: summary(data) to get a quick overview of the distribution of your variables.
* Ex: Visualizations: Scatter plots, box plots, histograms, and correlation matrices.

**Normality test(shapiro.test)** - The Shapiro-Wilk test is used to test for normality. It assesses whether a sample comes from a normally distributed population. Many statistical tests (like ANOVA and t-tests) assume that the data is normally distributed, so testing for normality is a common step.

When to Use:
Before running parametric tests like ANOVA, t-tests, or linear regression, where the assumption of normal distribution is crucial.

* Ex: You want to test if the sepal_length is normally distributed.

*Note:*
If the p-value is greater than 0.05, the data is approximately normal.
If the p-value is less than 0.05, the data significantly deviates from normality.

----------------------------------------

# Acknowledgements

I modified this code to create the code to read the csv in directly from github: https://lokraj.me/post/download-github-data/

I utilized this site for t.test clarification https://www.jmp.com/en_us/statistics-knowledge-portal/t-test.html

For the discussion of the purpose for normal distibutions and transforming data so it is normally distributed, the following article was used. 

Citation:
Mishra, P., Pandey, C. M., Singh, U., Gupta, A., Sahu, C., & Keshri, A. (2019). Descriptive statistics and normality tests for statistical data. Annals of cardiac anaesthesia, 22(1), 67–72. https://doi.org/10.4103/aca.ACA_157_18

Parts:

* Tutorial Introduction, objectives, and introduction were done by Kayla and Rebecca
* Instructions were done by Kayla with a couple packages added by Rebecca
* Importing the dataset was done by Kayla (first import code, head, tail, nrow, glimpse, NAs) and Rebecca (online download code, NAs)
* Jayden did the Looking into different variables section, Rebecca assisted with factoring code
* Exploratory Data Analysis section done by Kayla, Rebecca assised with filtering code
* Running Data Analysis section done by Rebecca
* Summary done by Jayden
* Formatting done by Rebecca
