---
title: "Group 1 Project 1 Tutorial"
output: html_document
date: "2024-09-29"
names: "Jayden Benzan, Rebecca Beneroff, Kayla Lichtner"
---
Tutorial: An Introduction to Data Preparation, Exploration, and Analysis

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Tutorial introduction 

Do you have a dataset but you're not sure where to start with analysis? This tutorial is for you! This tutorial utilizes *Iris* petal data. You will start by learning how to import, classify, and organize your data to prepare it for use. You will then perform some base level data exploration by looking at your variable's distributions assessing for normalcy, and transforming as necessary. Finally, you will develop and explore a question about the relationships between the variables presented in the data. Using this question, you will learn some of the different statistical tests that can be run, why these tests are run, and data visualization. 

###Tutorial objectives

There are three objectives for this tutorial:

1. Understand how to classify data (understanding different variable types and its effect on how to use them in analysis).
2. Review loading data, organizing data, and preparing data for statistical analysis.
3. Explain the conditions required for using linear modeling, t.tests, or ANOVA tests to analyze data. 

##Data introduction 
This tutorial's dataset highlights three species of iris plants: setosa, versicolor, and virginica. There are several variables reporting anatomical measurements such as sepal length, sepal width, sepal area, petal length, petal width, and petal area. In total, this dataset contains 1200 observations with 400 observations per species. 


### Instructions
## Loading the necessary packages 

```{r}
if (!require("UsingR")) install.packages("UsingR"); 
  library(UsingR) # For the simple.eda function
if (!require("cowplot")) install.packages("cowplot"); 
  library(cowplot) # For graphing aesthetics
if (!require("conflicted")) install.packages("conflicted"); 
  library (conflicted) # For conflicts
if (!require("readxl")) install.packages("readxl"); 
  library(readxl) #To read in data
if (!require("gplots")) install.packages("gplots"); 
  library(gplots) #For graphs/plots
if (!require("UsingR")) install.packages("UsingR"); 
  library(UsingR) #Generally good to have!
if (!require("tidyverse")) install.packages("tidyverse"); 
  library(tidyverse) # Always download this!
conflict_prefer_all("dplyr", quiet = TRUE)
```

## Importing the dataset

Go to our Git repository called Project_1_Group_1_2024, and download the "iris.extended.csv" data file from the "Dataset options" folder. We are going to make a final folder with the finished tutorial and the data csv file, but for now, please do this to import the data.

```{r}
iris.raw.data <- read.csv("iris_extended.csv")
```

Alternatively, you can use this script to download the data set from online:

```{r}
library (readr)
iris.raw.data <- read.csv(url("https://raw.githubusercontent.com/Bucknell-Biol364/Project_1_Group_1_2024/refs/heads/main/Working%20Folder/iris_extended.csv"))
```

*Citation for the dataset: https://www.kaggle.com/datasets/samybaladram/iris-dataset-extended?select=iris_extended.csv*

After loading in a dataset, you always want to make sure that everything loaded in properly. To do this, check the top and bottom of your data, and the number of observations you have imported. The data on GitHub reports 1200 lines of data, in our environment the data should read 1200 observations as there is one line for column headings.

```{r}
head(iris.raw.data)
tail(iris.raw.data)
nrow(iris.raw.data)
```
This shows us that there are 1200 rows of data which aligns with the expectation of 1200 lines of data according to Github.

It is important to understand the structure of the raw data as it will impact how you prepare for analysis. To do so:

```{r}
glimpse(iris.raw.data)
```

##Check for NAs in the data 

Now is a good time to check for NAs in the dataset, but there are no NAs in this data. 

## Looking into different variables- Jayden 


## Exploratory Data Analysis

It is important to run an initial exploratory data analysis to understand the distributions of your dataset as the distribution of data directly influences the type of statistical analyses to perform. 

To explore the distributions of some of the variables we are interested in we can use the simple.eda() function to look at the distributions of the numerical data.

```{r}
simple.eda(iris.raw.data$sepal_area)
simple.eda(iris.raw.data$sepal_length)
simple.eda(iris.raw.data$sepal_width)
simple.eda(iris.raw.data$petal_width)
simple.eda(iris.raw.data$petal_length)
simple.eda(iris.raw.data$petal_area)
```
The results of this preliminary analysis shows that sepal area and sepal width are approximately normally distributed as their histograms follow a bell shaped curve. The other variables sepal length, petal width, petal length, and petal area are not normally distributed as their histograms are skewed and their boxplots are skewed. Also, their data does not align well with the theoretical line in the Q-Q plot. 

To confirm the distributions of the dataset, we are going to use a Shapiro test for normality. To determine normality, if the p-value produced by a Shapiro test is greater than the confidence level alpha = 0.05, the distribution is normal. If the p-value produced by the Shapiro test is less than the confidence level alpha = 0.05, the data is not normally distributed. 
```{r}
shapiro.test(iris.raw.data$sepal_area)
shapiro.test(iris.raw.data$sepal_width)
shapiro.test(iris.raw.data$sepal_length)
shapiro.test(iris.raw.data$petal_length)
shapiro.test(iris.raw.data$petal_area)
shapiro.test(iris.raw.data$petal_width)
```
The results of the Shapiro test show that all six variables do not follow a normal distribution as the p-values produced by the Shapiro test are lower than the confidence level alpha = 0.05. Therefore, to prepare the data for statistical analysis, we will log transform the data to fit the data into a normal distribution.

To log transform the data
```{r}
iris.raw.data$log10sepal_length <- log10(iris.raw.data$sepal_length)
simple.eda(iris.raw.data$log10sepal_length)
iris.raw.data$log10petal_length <- log10(iris.raw.data$petal_length)
simple.eda(iris.raw.data$log10petal_length)
```
After log transforming the sepal length data, we find that it now follows a normal distribution. It is important for a dataset to be normally distributed for parametric tests as parametric tests use means to calculate statistical differences. Therefore, means need to be appropriately representative for a given data set, provided by a normal distribution.For instance a mean from a a given population that is skewed is not accurately representative of the data. 

After log transforming the petal length data, we found that it did not convert into a normal distribution. What aspect of the data do think is driving this? After reflection, please see below for an explanation!

Filter the data by species:
```{r}
setosa_data <- iris.raw.data |> 
  filter(species=="setosa")

versicolor_data <- iris.raw.data |>
  filter(species=="versicolor")

virginica_data <- iris.raw.data |>
  filter(species=="virginica")
```

Now, look at petal length for all three iris species separately:

```{r}
simple.eda(setosa_data$log10petal_length)
simple.eda(versicolor_data$log10petal_length)
simple.eda(virginica_data$log10petal_length)
```
By separating the data by species, we find that the log transformation of petal length is now normally distributed. 

----

Rebecca 

Stuff I needed to run my section

```{r}
iris.raw.data <- iris.raw.data |> 
  mutate(soil_type = as.factor(soil_type))

#summary(iris.raw.data$soil_type)
```

```{r}
iris.raw.data$log10sepal_length <- log10(iris.raw.data$sepal_length)
simple.eda(iris.raw.data$log10sepal_length)

iris.raw.data$log10sepal_width <- log10(iris.raw.data$sepal_width)
simple.eda(iris.raw.data$log10sepal_width)

iris.raw.data$log10petal_length<- log10(iris.raw.data$petal_length)
simple.eda(iris.raw.data$log10petal_length)

simple.eda(setosa_data$log10petal_length)
```

-----

## Running Data Analysis

### Form a Question:

At this point in your analysis you will be investigating the interactions between variables. You should start by formulating a question you might have about the dataset - For instance, you could choose to focus on one of the 3 species featured in the dataset. Noticing that *Iris setosa* inhabits three different soil types - loamy, sand, and clay - you might want to investigate if there is any relationship between soil type and sepal length in *I. setosa*. You could then look for relationships between soil and sepal width as well as soil and sepal total area. Maybe you want to extend that to elevation too. Once you have a question in mind you can decide what tests might best serve your investigation best. 

* Look at your data again and formulate a question or hypothesis you want to test:

```{r}
View(iris.raw.data)
```

### The T.Test

T.Tests are statistical tests that are used to compare the means between two groups of data. This is useful in  hypothesis testing because we can use a t.test to determine whether or not a factor or treatment actually has a significant effect on a population or group. It can also tell us whether or not two groups are different from one another. This test makes the following assumptions:

1. The data is continuous
2. The data set has been obtained by random samples from a population
3. The variability of the data in each group is similar
4. The distribution is approximately normal

To perform the t.test, you will first form a null hypothesis. The null hypothesis should state that what you are investigating has no significant effect on the population you are investigating. 

In the case of the Iris sepals, we will say that Soil type X does not have any affect on the length of *I. setosa* sepals. 

We are going to start by filtering our data into a new variable that only contains the *I. setosa* data. 

```{r}
setosa_data <- iris.raw.data |> 
  filter(species=="setosa")
#summary(iris.raw.data)

summary(setosa_data$soil_type)

```

Now for the sake of demonstration since we need 2 variables, lets say we're only interested in comparing loamy and sandy soils. We're conducting an experiment in house and we can't replicate clay, so we are only interested in loam and sand. We will then filter out clay from the data set. The | operator acts as "or" which means it will pull out loamy or sandy rows but not clay.

```{r}
setosa_soil_data <- setosa_data |> 
  filter(soil_type=="loamy" | soil_type=="sandy")
summary(setosa_soil_data$soil_type)
```
Perform any filtering or arranging you might need to do here:

```{r}

```


* State your null hypothesis:

You will now run the t.test. Use the below sepal/soil test code as a guide.

```{r}
t.test(sepal_length ~ soil_type, data = setosa_soil_data)
```

* Your Turn

```{r}

```

The example t.test returned a p-value of 0.7936. The p-value tells us the likelihood that you might observe the modeled outcome in your data set given that the null hypothesis is true. We will reject the null hypothesis if p < 0.05. Given our test returned a p-value of 0.7936, we can not confidently say amongst clay or sandy soil that soil type affects the length of sepals in *I. setosa* so we fail to reject the null hypothesis.

* Interpret your t.test below:


### Linear Modeling

Linear modeling looks for direct correlations between variables. 
It is a parametric test and assumes: 

1. The relationship between x and the mean of y is linear
2. That the observed data points are independent of each other
3. That the data is normally distributed
4. Homoscedasticity - That the spread of data variance is approximately the same

So if we want to look at our soil and sepal question, we can create a linear model that tells us 

```{r}
lmSoilSepal <- lm(log10sepal_length ~ soil_type, data = setosa_data)

lmSoilSepal

summary(lmSoilSepal)
```
Now what does this look like? We can make a quick graph to take a look at our data

```{r}
ggplot(setosa_data, aes(x = soil_type, y = log10sepal_length)) +
  geom_point(position = "jitter",size=0.05) +
  scale_y_continuous(breaks = seq.int(0, 20, 2)) +
  ggtitle('Sepal Length relationship with soil type') +
  theme_cowplot()

```



-----------------------------

### Linear Modeling

Linear modeling looks for direct correlations between variables. 
It is a parametric test and assumes: 

1. The relationship between x and the mean of y is linear
1. That the observed data points are independant of each other
1. That the data is normally distributed
1. Homoscedasticity - That the spread of data variance is approximately the same

Lets investigate a new question, since there was no significance in sepal/soil data, lets investigate the relationship between sepal length and width. We can investigate this with a linear model.


```{r}
lmSepalLW <- lm(log10sepal_length ~ log10sepal_width, data = setosa_data)
lmSepalLW

summary(lmSepalLW)
```
Now what does this look like? We can make a quick graph to take a look at our data

```{r}
ggplot(setosa_data) +
  aes(x = log10petal_length, y=log10sepal_length) +
  geom_point(position = "jitter",size=1) +
  theme_cowplot()+
   geom_smooth(method='lm',se=FALSE)

```

There is no relationship between 

----------------------------------------

### Analysis of Variance (ANOVA) Test

Anova tests follow the same principle as t-tests but they are used when you have more than 2 groups. It can also be used to compare nested models.

So, returning to our soil and sepal tests we can add clay back into the mix! We will therefore use our filtered setosa data set rather than the soil filtered set.

```{r}

```


## Acknowledgements

I modified this code to create the code to read the csv in directly from github: https://lokraj.me/post/download-github-data/

I utilized this site for t.test clarification https://www.jmp.com/en_us/statistics-knowledge-portal/t-test.html

For the discussion of the purpose for normal distibutions and transforming data so it is normally distributed, the following article was used. 

Citation:
Mishra, P., Pandey, C. M., Singh, U., Gupta, A., Sahu, C., & Keshri, A. (2019). Descriptive statistics and normality tests for statistical data. Annals of cardiac anaesthesia, 22(1), 67–72. https://doi.org/10.4103/aca.ACA_157_18
